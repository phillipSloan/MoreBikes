{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c31d6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e55d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan(df):\n",
    "    # get a list of all the columns containing NaN\n",
    "    nan_cols = df[df.columns[df.isnull().any()]].columns\n",
    "    nan_cols = nan_cols.drop('bikes')\n",
    "    # compute and fill each NaN with the columns mean\n",
    "    df[nan_cols] = df[nan_cols].fillna(value=df[nan_cols].mean())\n",
    "\n",
    "    \n",
    "def show_nans(df):\n",
    "    print(np.unique(df['station']))\n",
    "    print(df.shape[0] - df.dropna().shape[0])\n",
    "#     print(df[df.columns[df.isnull().any()]].columns)\n",
    "    print(df.isnull().any())\n",
    "    print()\n",
    "    \n",
    "\n",
    "# converting weekdays into integers [1-7]\n",
    "def convert_weekdays(df):\n",
    "    df = df.replace(\n",
    "    ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "    [1, 2, 3, 4, 5, 6, 7])\n",
    "    return df\n",
    "    \n",
    "def score_abs_error(model, data, num_docks, round_ = False):\n",
    "    if round_ == True:\n",
    "        y_pred = np.around(  model.predict(data.iloc[:,:-1].to_numpy()))# * num_docks  )\n",
    "    else:\n",
    "        y_pred = model.predict(data.iloc[:,:-1].to_numpy())# * num_docks\n",
    "    y_gold = data[\"bikes\"].to_numpy()# * num_docks\n",
    "    \n",
    "    return mean_absolute_error(y_gold, y_pred)\n",
    "\n",
    "def reasonable_predictions(model, data):\n",
    "    y_pred = model.predict(data.to_numpy())\n",
    "    \n",
    "    y_pred = np.around(y_pred)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d7c33d",
   "metadata": {},
   "source": [
    "## This code is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452c3c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 52]\n",
      "found val set two\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def is_hol_weekend(row):\n",
    "    if row['weekday'] == 6 or row['weekday'] == 7:\n",
    "        return 1\n",
    "    if row['isHoliday'] == 1:\n",
    "        return 1\n",
    "    if row['hour'] > 17 or row['hour'] < 9:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def generate_dataframe(dataframe):\n",
    "    dataframe = convert_weekdays(dataframe)\n",
    "    \n",
    "    # # deleting unneeded columns\n",
    "    del dataframe[\"month\"]\n",
    "    del dataframe[\"year\"]\n",
    "    # del df[\"timestamp\"]\n",
    "    del dataframe[\"station\"]\n",
    "    del dataframe[\"precipitation.l.m2\"]\n",
    "    \n",
    "    del dataframe[\"latitude\"]\n",
    "    del dataframe[\"longitude\"]\n",
    "    \n",
    "    \n",
    "    default_columns = list(dataframe.columns)\n",
    "    \n",
    "    dataframe['isOff'] = dataframe.apply(is_hol_weekend,axis=1)\n",
    "    \n",
    "    default_columns = [\"isOff\"] + default_columns\n",
    "    \n",
    "    dataframe = dataframe[default_columns]\n",
    "    \n",
    "    print(dataframe.columns)\n",
    "    \n",
    "    del dataframe['isHoliday']\n",
    "    #del dataframe[\"timestamp\"]\n",
    "    \n",
    "    \n",
    "    columns = list(dataframe.columns[-6:])\n",
    "    \n",
    "    if \"bikes\" in columns:\n",
    "        pass\n",
    "    else:\n",
    "        columns = columns[1:]\n",
    "    \n",
    "#     print(columns)\n",
    "#     for c in columns:\n",
    "#         dataframe[c] = dataframe[c].to_numpy() / dataframe[\"numDocks\"].to_numpy()\n",
    "    num_docks = dataframe[\"numDocks\"]\n",
    "    \n",
    "    del dataframe[\"numDocks\"]\n",
    "    \n",
    "    return dataframe, num_docks\n",
    "\n",
    "def vectorise_dataframe(dataframe):\n",
    "    \n",
    "    # # deleting unneeded columns\n",
    "    del dataframe[\"month\"]\n",
    "    del dataframe[\"year\"]\n",
    "    #del dataframe[\"timestamp\"]\n",
    "    del dataframe[\"station\"]\n",
    "    del dataframe[\"precipitation.l.m2\"]\n",
    "    #del dataframe[\"weekhour\"]\n",
    "    \n",
    "    del dataframe[\"latitude\"]\n",
    "    del dataframe[\"longitude\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    default_columns = list(dataframe.columns)\n",
    "    #default_columns.remove('weekday')\n",
    "    dataframe['isOff'] = dataframe.apply(is_hol_weekend,axis=1)\n",
    "    \n",
    "    #dataframe = pd.get_dummies(dataframe, columns=['weekday'])\n",
    "    \n",
    "    #default_columns = [\"isOff\", \"weekday_Sunday\", \"weekday_Monday\", \"weekday_Tuesday\", \"weekday_Wednesday\", \n",
    "                      #\"weekday_Thursday\", \"weekday_Friday\", \"weekday_Saturday\"] + default_columns\n",
    "    \n",
    "    default_columns = [\"isOff\"] + default_columns\n",
    "    #del dataframe[\"weekday\"]\n",
    "    default_columns.remove('weekday')\n",
    "    #print(default_columns)\n",
    "    \n",
    "    \n",
    "    dataframe = dataframe[default_columns]\n",
    "    \n",
    "    #print(dataframe.columns)\n",
    "    \n",
    "    del dataframe['isHoliday']\n",
    "    #del dataframe[\"timestamp\"]\n",
    "    \n",
    "    \n",
    "    columns = list(dataframe.columns[-6:])\n",
    "    \n",
    "    if \"bikes\" in columns:\n",
    "        pass\n",
    "    else:\n",
    "        columns = columns[1:]\n",
    "    \n",
    "#     #print(columns)\n",
    "#     for c in columns:\n",
    "#         dataframe[c] = dataframe[c].to_numpy() / dataframe[\"numDocks\"].to_numpy()\n",
    "    num_docks = dataframe[\"numDocks\"]\n",
    "    \n",
    "    dataframe[\"numDocks\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     columns_kept = [\"weekday\", \"hour\", \"isOff\",  'full_profile_3h_diff_bikes', \n",
    "#                      'short_profile_3h_diff_bikes',  \"bikes\"] #'bikes_3h_ago',\n",
    "    \n",
    "#     for c in dataframe.columns:\n",
    "#         if c not in columns_kept:\n",
    "#             del dataframe[c]\n",
    "    \n",
    "    \n",
    "    return dataframe, num_docks\n",
    "\n",
    "# Adding all files into one DataFrame\n",
    "trains = []\n",
    "vals = []\n",
    "\n",
    "train_docks_list = []\n",
    "val_docks_list = []\n",
    "\n",
    "scalers = []\n",
    "count = 0\n",
    "for i, path in enumerate(Path('./Train/Train').rglob('*.csv')):\n",
    "    count += 1\n",
    "\n",
    "val_inds = list(np.random.randint(0, count - 1, 2))\n",
    "print(val_inds)\n",
    "\n",
    "for i, path in enumerate(Path('./Train/Train').rglob('*.csv')):\n",
    "    tmp = pd.read_csv(path)\n",
    "\n",
    "    tmp = tmp.dropna(axis='rows')\n",
    "    \n",
    "    #print(tmp.columns)\n",
    "\n",
    "    \n",
    "    if i  not in  val_inds:\n",
    "        train, val = train_test_split(tmp, test_size=0.02)\n",
    "\n",
    "        train, train_docks = vectorise_dataframe(train)\n",
    "        val, val_docks = vectorise_dataframe(val)\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        train[train.columns[:-1]] = scaler.fit_transform(train[train.columns[:-1]])\n",
    "        val[val.columns[:-1]] = scaler.transform(val[val.columns[:-1]])\n",
    "\n",
    "        trains.append(train)\n",
    "        vals.append(val)\n",
    "\n",
    "        train_docks_list.append(train_docks)\n",
    "        val_docks_list.append(val_docks)\n",
    "\n",
    "        scalers.append(scaler)\n",
    "        \n",
    "        del tmp\n",
    "    elif i == val_inds[0]:\n",
    "        val_set = tmp.copy()\n",
    "\n",
    "\n",
    "        val_set, val_set_docks = vectorise_dataframe(val_set)\n",
    "    else:\n",
    "        print('found val set two')\n",
    "        val_set_two = tmp.copy()\n",
    "\n",
    "\n",
    "        val_set_two, val_set_docks_two = vectorise_dataframe(val_set_two)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "print(len(trains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f46ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forest = RandomForestRegressor(n_estimators=500, n_jobs=6)\n",
    "# print(\"initialised\")\n",
    "# forest.fit(train.iloc[:,:-1].to_numpy(), train[\"bikes\"].to_numpy())\n",
    "\n",
    "# importances = forest.feature_importances_\n",
    "# imp_indixes = np.argsort(importances)[::-1]\n",
    "# feature_order = train.columns[:-1][imp_indixes]\n",
    "# importances = importances[imp_indixes]\n",
    "\n",
    "# imp_df = pd.DataFrame(data = importances, index = feature_order, columns=[\"relative_importance\"])\n",
    "\n",
    "# print(score_abs_error(forest, val, val_docks))\n",
    "\n",
    "# print(imp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ba5b0",
   "metadata": {},
   "source": [
    "# Random elimination parameter tuning\n",
    "## Random forest regressor\n",
    "\n",
    "This cell uses `HalvingRandomSearchCV` to find near-optimal parameters for a random forest regressor. It takes a while to run with these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72444e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialised\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 73/73 [00:24<00:00,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted in 24.16706418991089s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from tqdm import tqdm\n",
    "start = time()\n",
    "print(\"initialised\")\n",
    "\n",
    "models = []\n",
    "\n",
    "for i in tqdm(range(len(trains))):\n",
    "    #forest_boost = AdaBoostRegressor(n_estimators=400, random_state=0, learning_rate = 0.5)\n",
    "    forest_boost = RandomForestRegressor(n_estimators= 200, max_depth= 9)\n",
    "    #forest_boost = GradientBoostingRegressor(n_estimators= 150, max_depth= 3, learning_rate= 0.1, loss=\"squared_error\")#\n",
    "    forest_boost.fit(trains[i].iloc[:,:-1].to_numpy(), trains[i][\"bikes\"].to_numpy())\n",
    "    \n",
    "    models.append(forest_boost)\n",
    "print(f'fitted in {time() - start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c6d2416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding all files into one DataFrame\n",
    "# converting weekdays into integers [1-7]\n",
    "def convert_weekdays(df):\n",
    "    df = df.replace(\n",
    "    ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "    [1, 2, 3, 4, 5, 6, 7], inplace=True)\n",
    "\n",
    "\n",
    "df = []\n",
    "for path in Path('./Train/Train').rglob('*.csv'):\n",
    "    tmp = pd.read_csv(path)\n",
    "    # comment next line if not averaging NaNs  \n",
    "    # show_nans(tmp)\n",
    "    #replace_nan(tmp)\n",
    "    tmp = tmp.dropna(axis='rows')\n",
    "    df.append(tmp)\n",
    "\n",
    "df = pd.concat(df, ignore_index=True)\n",
    "\n",
    "train, val = train_test_split(df, test_size=0.02)\n",
    "\n",
    "train, train_docks = vectorise_dataframe(train)\n",
    "val, val_docks = vectorise_dataframe(val)\n",
    "\n",
    "x = train.iloc[:,:-1].to_numpy()\n",
    "y = train['bikes'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1101799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Adding all files into one DataFrame\n",
    "all_columns = list(train.columns)\n",
    "\n",
    "errors = []\n",
    "\n",
    "for path in Path('./Models/Models').rglob('*.csv'):\n",
    "    tmp = pd.read_csv(path)\n",
    "    # comment next line if not averaging NaNs  \n",
    "    # show_nans(tmp)\n",
    "    #replace_nan(tmp)\n",
    "    tmp = tmp.dropna(axis='rows')\n",
    "    features = tmp['feature']\n",
    "    weights = tmp['weight']\n",
    "    \n",
    "    weight_based = np.zeros((1, len(all_columns)))\n",
    "    model_frame = pd.DataFrame(data=weight_based, columns=all_columns)\n",
    "    \n",
    "    for i, f in enumerate(features):\n",
    "        model_frame[f] = weights[i]\n",
    "    \n",
    "    # removing intercept and bikes\n",
    "    final_weights = model_frame.iloc[0,:-2].to_numpy()\n",
    "    \n",
    "    intercept = model_frame.iloc[0,-1]\n",
    "    \n",
    "    model = linear_model.LinearRegression()\n",
    "    model.coef_ = final_weights\n",
    "    model.intercept_ = intercept    \n",
    "    \n",
    "    models.append(model)\n",
    "    \n",
    "    trains.append(train)\n",
    "    vals.append(val)\n",
    "    \n",
    "    train_docks_list.append(0)\n",
    "    val_docks_list.append(0)\n",
    "    \n",
    "    #y_pred = model.predict(x)\n",
    "    #errors.append(mean_absolute_error(y, y_pred))\n",
    "    \n",
    "    scalers.append(None)\n",
    "    \n",
    "    # print(final_weights)\n",
    "    # print(intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0056a950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▍                                       | 73/1273 [00:01<00:18, 64.84it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lx/p5f6tjxn78d7x0yr11l1pm_h0000gn/T/ipykernel_12413/3254763482.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_abs_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_docks_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mrounded_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_abs_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_docks_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#errors = []\n",
    "rounded_errors = []\n",
    "y_pred = []\n",
    "y_gold = []\n",
    "\n",
    "for i in tqdm(range(len(vals))):\n",
    "    errors.append(score_abs_error(models[i], vals[i], val_docks_list[i]))\n",
    "    rounded_errors.append(score_abs_error(models[i], vals[i], val_docks_list[i], round_ = True))\n",
    "    \n",
    "    pred = list(models[i].predict(vals[i].iloc[:, :-1].to_numpy()))# * val_docks_list[i])\n",
    "    \n",
    "    y_pred = y_pred + pred\n",
    "    \n",
    "    y_gold = y_gold + list(vals[i][\"bikes\"])# * val_docks_list[i])\n",
    "    \n",
    "print(f'Non-rounded ensemble error: {np.mean(errors)}')\n",
    "print(f'Rounded ensemble error: {np.mean(rounded_errors)}')\n",
    "\n",
    "plt.hist(np.array(y_pred) - np.array(y_gold), bins=30)\n",
    "plt.show()\n",
    "plt.hist(errors, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff74a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_best_models = np.argsort(errors)\n",
    "#print(models)\n",
    "best_model_ind = inds_best_models[0]\n",
    "best_model = models[best_model_ind]\n",
    "#print(best_model)\n",
    "\n",
    "\n",
    "\n",
    "best_pred = best_model.predict(val_set.iloc[:, :-1].to_numpy())# * val_set_docks\n",
    "val_gold = val_set[\"bikes\"]# * val_set_docks\n",
    "\n",
    "print(f'Error on validation holdout: {mean_absolute_error(val_gold, best_pred)}')\n",
    "\n",
    "plt.hist(np.array(val_gold) - np.array(best_pred), bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e38ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = np.array(errors)\n",
    "model_weights = 1/ model_weights**(2)\n",
    "model_weights = model_weights / np.sum(model_weights)\n",
    "# should equal 1\n",
    "print(np.sum(model_weights))\n",
    "\n",
    "plt.hist(model_weights, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8674f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_validation_errors(models, scalers, x, y,  docks):\n",
    "    results = np.zeros(len(models))\n",
    "    \n",
    "    for i, m in enumerate(models):\n",
    "        \n",
    "        if scalers[i] != None:\n",
    "            x_i = scalers[i].transform(x)\n",
    "        \n",
    "        results[i] = mean_absolute_error(m.predict(x_i),y)# * docks, y*docks)\n",
    "\n",
    "    return results\n",
    "\n",
    "#print(trains[0].columns)\n",
    "#print(val_set.columns)\n",
    "print(scalers[0].feature_names_in_)\n",
    "\n",
    "validation_errors = ensemble_validation_errors(models, scalers, val_set.iloc[:, :-1], val_set[\"bikes\"], val_set_docks)\n",
    "\n",
    "print(f'Validation error pre re-tuning:  {np.mean(validation_errors)}')\n",
    "\n",
    "model_weights = model_weights / validation_errors**2\n",
    "model_weights = model_weights / np.sum(model_weights)\n",
    "\n",
    "print(models[0].feature_importances_)\n",
    "\n",
    "\n",
    "plt.hist(model_weights, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f43683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, scalers, model_weights, x):\n",
    "    results = np.zeros(x.shape[0])\n",
    "\n",
    "    for i, m in enumerate(models):\n",
    "            \n",
    "        if scalers[i] != None:\n",
    "            x_i = scalers[i].transform(x)\n",
    "        \n",
    "        results = results + m.predict(x_i) * model_weights[i]\n",
    "\n",
    "    return results\n",
    "\n",
    "y_pred = ensemble_predict(models, scalers, model_weights, val_set_two.iloc[:,:-1]) #* val_set_docks_two\n",
    "\n",
    "val_gold_two = val_set_two[\"bikes\"].to_numpy()# * val_set_docks_two\n",
    "\n",
    "print(mean_absolute_error(val_gold_two, y_pred))\n",
    "\n",
    "plt.hist(val_gold_two - y_pred, bins=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ec672d",
   "metadata": {},
   "source": [
    "(n_estimators= 100, min_samples_leaf= 5, max_depth= 5, learning_rate= 0.025, loss=\"absolute_error\") = 6.171106155700234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167576aa",
   "metadata": {},
   "source": [
    "## Score tracking\n",
    "\n",
    " -  vectorized data, n_estimators= 10000, min_samples_leaf= 5, max_depth= 5, learning_rate= 0.005, verbose = 1, loss=\"absolute_error\", score 2.31, rounding integers 2.29\n",
    " \n",
    " - un-vectorized data, n_estimators= 10000, min_samples_leaf= 5, max_depth= 5, learning_rate= 0.005, verbose = 1, loss=\"absolute_error\", score 2.33, rounding integers 2.31\n",
    " \n",
    " - vectorized data, n_estimators= 1000, min_samples_leaf= 5, max_depth= 5, learning_rate= 0.005, 2.43, 2.42\n",
    " \n",
    " - un-vectorized data, n_estimators= 1000, min_samples_leaf= 5, max_depth= 5, learning_rate= 0.005, , loss=\"absolute_error\",  2.4, 2.38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f1044",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# results = pd.DataFrame(search.cv_results_)\n",
    "# results[\"params_str\"] = results.params.apply(str)\n",
    "# params = search.param_distributions\n",
    "# # results.drop_duplicates(subset=(\"params_str\", \"iter\"), inplace=True)\n",
    "# learning_rates = params[\"learning_rate\"]\n",
    "# mean_scores = results.pivot(\n",
    "#     index=\"iter\", columns=\"params_str\", values=\"mean_test_score\"\n",
    "# )\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(16,12))\n",
    "# mean_scores.plot(legend=False, alpha=0.6, ax = ax, linewidth=8)\n",
    "\n",
    "# labels = [\n",
    "#     f\"iter={i}\\nn_samples={search.n_resources_[i]} \\nn_candidates={search.n_candidates_[i]}\"# \\nn_estimators={params[\"n_estimators\"][i]} \"\n",
    "#     for i in range(search.n_iterations_)\n",
    "# ]\n",
    "\n",
    "# ax.set_xticks(range(search.n_iterations_))\n",
    "# ax.set_xticklabels(labels, rotation=45, multialignment=\"left\")\n",
    "# ax.set_title(\"Scores of candidates over iterations\")\n",
    "# ax.set_ylabel(\"mean test score\", fontsize=15)\n",
    "# ax.set_xlabel(\"iterations\", fontsize=15)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# n_est = 10000\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# test_score = np.zeros((n_est,), dtype=np.float64)\n",
    "\n",
    "# y_test = val[\"bikes\"]\n",
    "# #y_pred = reasonable_predictions(forest_boost, val.iloc[:, :-1])\n",
    "\n",
    "# for i, y_pred in enumerate(forest_boost.staged_predict(val.iloc[:,:-1])):\n",
    "#     test_score[i] = forest_boost.loss_(y_test, y_pred)\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(12\n",
    "#                           , 12))\n",
    "# plt.subplot(1, 1, 1)\n",
    "# plt.title(\"Training and validation error\")\n",
    "# plt.plot(\n",
    "#     np.arange(n_est) + 1,\n",
    "#     forest_boost.train_score_,\n",
    "#     \"b-\",\n",
    "#     label=\"Training Set\",\n",
    "# )\n",
    "# plt.plot(\n",
    "#     np.arange(n_est) + 1, test_score, \"r-\", label=\"Validation Set\"\n",
    "# )\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel(\"Boosting Iterations\")\n",
    "# plt.ylabel(\"Absolute error\")\n",
    "# #plt.yscale('log')\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8,8))\n",
    "# plt.hist(y_test*val_docks - forest_boost.predict(val.iloc[:,:-1])*val_docks, bins = 30)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0590cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "ids = test[\"Id\"]\n",
    "\n",
    "del test[\"Id\"]\n",
    "# del test[\"month\"]\n",
    "# del test[\"year\"]\n",
    "\n",
    "# convert_weekdays(test)\n",
    "# # test[test.columns] = scaler.fit_transform(test[test.columns])\n",
    "\n",
    "# for feature in lowest_ranked_10:\n",
    "#     del test[feature]\n",
    "    \n",
    "# print(test.columns)  \n",
    "# print(train.columns)\n",
    "\n",
    "test, test_docks = vectorise_dataframe(test)\n",
    "\n",
    "#y_pred = forest_boost.predict(test)\n",
    "y_pred = np.around(ensemble_predict(models, scalers, model_weights, test)).astype(np.int32)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "sub_df = pd.DataFrame(data=y_pred, index = ids, columns = [\"bikes\"])\n",
    "\n",
    "sub_df.index.name = 'Id'\n",
    "\n",
    "print(sub_df.head())\n",
    "\n",
    "sub_df.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d6d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
