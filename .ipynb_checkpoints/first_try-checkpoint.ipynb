{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e294905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc68a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan(df):\n",
    "    # get a list of all the columns containing NaN\n",
    "    nan_cols = df[df.columns[df.isnull().any()]].columns\n",
    "    nan_cols = nan_cols.drop('bikes')\n",
    "    # compute and fill each NaN with the columns mean\n",
    "    df[nan_cols] = df[nan_cols].fillna(value=df[nan_cols].mean())\n",
    "\n",
    "    \n",
    "def show_nans(df):\n",
    "    print(np.unique(df['station']))\n",
    "#     print(df[df.columns[df.isnull().any()]].columns)\n",
    "    print(df.isnull().any())\n",
    "    print()\n",
    "    \n",
    "\n",
    "# converting weekdays into integers [1-7]\n",
    "def convert_weekdays(df):\n",
    "    df = df.replace(\n",
    "    ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "    [1, 2, 3, 4, 5, 6, 7], inplace=True)\n",
    "    \n",
    "def score_abs_error(model, data, round_ = False):\n",
    "    if round_ == True:\n",
    "        y_pred = np.around(  model.predict(data.iloc[:,:-1].to_numpy())  )\n",
    "    else:\n",
    "        y_pred = model.predict(data.iloc[:,:-1].to_numpy())\n",
    "    y_gold = data[\"bikes\"].to_numpy()\n",
    "    \n",
    "    return mean_absolute_error(y_gold, y_pred)\n",
    "\n",
    "def reasonable_predictions(model, data):\n",
    "    y_pred = model.predict(data.to_numpy())\n",
    "    \n",
    "    y_pred = np.around(y_pred)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1b21ed",
   "metadata": {},
   "source": [
    "## This code is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fe805b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13275"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding all files into one DataFrame\n",
    "df = []\n",
    "for path in Path('./Train/Train').rglob('*.csv'):\n",
    "    tmp = pd.read_csv(path)\n",
    "    # comment next line if not averaging NaNs  \n",
    "#     show_nans(tmp)\n",
    "#     replace_nan(tmp)\n",
    "#     tmp = tmp.dropna(axis='rows')\n",
    "    df.append(tmp)\n",
    "\n",
    "df = pd.concat(df, ignore_index=True)\n",
    "df.shape[0] - df.dropna().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding all files into one DataFrame\n",
    "df = []\n",
    "for path in Path('./Train/Train').rglob('*.csv'):\n",
    "    tmp = pd.read_csv(path)\n",
    "    # comment next line if not averaging NaNs  \n",
    "    show_nans(tmp)\n",
    "    replace_nan(tmp)\n",
    "    tmp = tmp.dropna(axis='rows')\n",
    "    df.append(tmp)\n",
    "\n",
    "df = pd.concat(df, ignore_index=True)\n",
    "\n",
    "convert_weekdays(df)\n",
    "\n",
    "# deleting unneeded columns\n",
    "del df[\"month\"]\n",
    "del df[\"year\"]\n",
    "\n",
    "# comment next line if not dropping NaNs\n",
    "\n",
    "columns = list(df.columns[-6:-1])\n",
    "print(columns)\n",
    "for c in columns:\n",
    "    df[c] = df[c].to_numpy() / df[\"numDocks\"].to_numpy()\n",
    "    \n",
    "print(df.head())\n",
    "\n",
    "# See all Rows/Cols\n",
    "# pd.set_option('display.max_columns', 23)\n",
    "pd.set_option('display.max_rows', 23)\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# df[df.columns[:-1]] = scaler.fit_transform(df[df.columns[:-1]])\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c675465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df[\"bikes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b0091",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor(n_estimators=500, n_jobs=6)\n",
    "print(\"initialised\")\n",
    "forest.fit(df.iloc[:,:-1].to_numpy(), df[\"bikes\"].to_numpy())\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "imp_indixes = np.argsort(importances)[::-1]\n",
    "feature_order = df.columns[:-1][imp_indixes]\n",
    "importances = importances[imp_indixes]\n",
    "\n",
    "imp_df = pd.DataFrame(data = importances, index = feature_order, columns=[\"relative_importance\"])\n",
    "\n",
    "print(score_abs_error(forest, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaddfe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(imp_df)\n",
    "# X = df.iloc[:,:-1].to_numpy()\n",
    "# Y = df[\"bikes\"].to_numpy()\n",
    "\n",
    "# from sklearn.decomposition in_estimators=t PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# pca = PCA(n_components = 2, whiten = True)\n",
    "# X_pca = pca.fit_transform(X)\n",
    "\n",
    "# print(X_pca.shape)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "# ax.scatter(X_pca[:,0], X_pca[:,1], c = Y, edgecolor = '0', alpha=0.5)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# print(np.corrcoef(X_pca.transpose(), Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# bins = np.linspace(0, np.max(Y), 8)\n",
    "# digitized = np.digitize(Y, bins)\n",
    "# bin_means = [Y[digitized == i].mean() for i in range(1, len(bins))]\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12,12))\n",
    "# sns.kdeplot(\n",
    "#     x=X_pca[:,0], y=X_pca[:,1], ax = ax, warn_singular=False, fill = True, hue=digitized\n",
    "# )#\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75faf89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_features = feature_order[:5].to_list()\n",
    "# plot_features.append(\"bikes\")\n",
    "# print(plot_features)\n",
    "\n",
    "# g = sns.PairGrid(df[plot_features], diag_sharey=False, corner=True)\n",
    "# g.map_upper(sns.scatterplot)\n",
    "# g.map_lower(sns.kdeplot)\n",
    "# g.map_diag(sns.kdeplot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f898b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lowest_ranked_10 = feature_order[-5:]\n",
    "for feature in lowest_ranked_10:\n",
    "    del df[feature]\n",
    "print(imp_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e870419",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890868d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aff4744b",
   "metadata": {},
   "source": [
    "# Random elimination parameter tuning\n",
    "## Random forest regressor\n",
    "\n",
    "This cell uses `HalvingRandomSearchCV` to find near-optimal parameters for a random forest regressor. It takes a while to run with these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d011d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "\n",
    "# forest_boost = GradientBoostingRegressor(n_estimators=n_est, loss='squared_error', learning_rate=0.2, max_depth=2, verbose=1)\n",
    "#  forest_boost = SVR()\n",
    "searched_boost = RandomForestRegressor()\n",
    "searched_boost = GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "\n",
    "param_distributions = {\"max_depth\":  [2,3,4],   #, 5, 6, None],\n",
    "                       \"min_samples_split\": np.around(np.linspace(2,20,10)).astype(np.int32),\n",
    "                       \"learning_rate\": np.linspace(0.0001,1,10),\n",
    "                       \"n_estimators\": np.linspace(5, 5000, 50).astype(np.int32)\n",
    "                      }\n",
    "\n",
    "search = HalvingRandomSearchCV(searched_boost, param_distributions,\n",
    "                               resource='n_samples', aggressive_elimination=True, min_resources = 1000,\n",
    "                               factor = 2 ,cv = 2,    # n_candidates =  25\n",
    "                               random_state=0, verbose=1, n_jobs=6).fit(train.iloc[:,:-1].to_numpy(), train[\"bikes\"].to_numpy())\n",
    "\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "print(\"initialised\")\n",
    "#forest_boost = GradientBoostingRegressor(**search.best_params_)\n",
    "forest_boost = GradientBoostingRegressor(n_estimators= 5000, min_samples_split= 8, max_depth= 3, learning_rate= 0.11120000000000001, verbose = 1)\n",
    "forest_boost.fit(train.iloc[:,:-1].to_numpy(), train[\"bikes\"].to_numpy())\n",
    "print(f'fitted in {time() - start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_abs_error(forest_boost, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51759c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results = pd.DataFrame(search.cv_results_)\n",
    "results[\"params_str\"] = results.params.apply(str)\n",
    "params = search.param_distributions\n",
    "# results.drop_duplicates(subset=(\"params_str\", \"iter\"), inplace=True)\n",
    "learning_rates = params[\"learning_rate\"]\n",
    "mean_scores = results.pivot(\n",
    "    index=\"iter\", columns=\"params_str\", values=\"mean_test_score\"\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,12))\n",
    "mean_scores.plot(legend=False, alpha=0.6, ax = ax, linewidth=8)\n",
    "\n",
    "labels = [\n",
    "    f\"iter={i}\\nn_samples={search.n_resources_[i]} \\nn_candidates={search.n_candidates_[i]}\"# \\nn_estimators={params[\"n_estimators\"][i]} \"\n",
    "    for i in range(search.n_iterations_)\n",
    "]\n",
    "\n",
    "ax.set_xticks(range(search.n_iterations_))\n",
    "ax.set_xticklabels(labels, rotation=45, multialignment=\"left\")\n",
    "ax.set_title(\"Scores of candidates over iterations\")\n",
    "ax.set_ylabel(\"mean test score\", fontsize=15)\n",
    "ax.set_xlabel(\"iterations\", fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5cc553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     n_est = search.best_params_[\"n_estimators\"]\n",
    "# except:\n",
    "n_est = 5000\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "test_score = np.zeros((n_est,), dtype=np.float64)\n",
    "\n",
    "y_test = val[\"bikes\"]\n",
    "#y_pred = reasonable_predictions(forest_boost, val.iloc[:, :-1])\n",
    "\n",
    "for i, y_pred in enumerate(forest_boost.staged_predict(val.iloc[:,:-1])):\n",
    "    test_score[i] = forest_boost.loss_(y_test, np.around(y_pred))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title(\"Training and validation error\")\n",
    "plt.plot(\n",
    "    np.arange(n_est) + 1,\n",
    "    forest_boost.train_score_,\n",
    "    \"b-\",\n",
    "    label=\"Training Set\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(n_est) + 1, test_score, \"r-\", label=\"Validation Set\"\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting Iterations\")\n",
    "plt.ylabel(\"Squared error\")\n",
    "plt.yscale('log')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd587e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(y_test - forest_boost.predict(val.iloc[:,:-1]), bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6599ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "ids = test[\"Id\"]\n",
    "\n",
    "del test[\"Id\"]\n",
    "del test[\"month\"]\n",
    "del test[\"year\"]\n",
    "\n",
    "convert_weekdays(test)\n",
    "# test[test.columns] = scaler.fit_transform(test[test.columns])\n",
    "\n",
    "for feature in lowest_ranked_10:\n",
    "    del test[feature]\n",
    "    \n",
    "print(test.columns)  \n",
    "print(train.columns)\n",
    "\n",
    "#y_pred = forest_boost.predict(test)\n",
    "y_pred = reasonable_predictions(forest_boost, test)\n",
    "\n",
    "sub_df = pd.DataFrame(data=y_pred, index = ids, columns = [\"bikes\"])\n",
    "\n",
    "sub_df.index.name = 'Id'\n",
    "\n",
    "print(sub_df.head())\n",
    "\n",
    "sub_df.to_csv(\"trial.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2981ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
