{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31929b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c473f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan(df):\n",
    "    # get a list of all the columns containing NaN\n",
    "    nan_cols = df[df.columns[df.isnull().any()]].columns\n",
    "    nan_cols = nan_cols.drop('bikes')\n",
    "    # compute and fill each NaN with the columns mean\n",
    "    df[nan_cols] = df[nan_cols].fillna(value=df[nan_cols].mean())\n",
    "\n",
    "    \n",
    "def show_nans(df):\n",
    "    print(np.unique(df['station']))\n",
    "    print(df.shape[0] - df.dropna().shape[0])\n",
    "#     print(df[df.columns[df.isnull().any()]].columns)\n",
    "    print(df.isnull().any())\n",
    "    print()\n",
    "    \n",
    "\n",
    "# converting weekdays into integers [1-7]\n",
    "def convert_weekdays(df):\n",
    "    df = df.replace(\n",
    "    ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
    "    [1, 2, 3, 4, 5, 6, 7], inplace=True)\n",
    "    \n",
    "def score_abs_error(model, data, n_docks, round_ = False):\n",
    "    \n",
    "    print(data.shape)\n",
    "    print(n_docks.shape)\n",
    "    \n",
    "    if round_ == True:\n",
    "        y_pred = np.around(  model.predict(data.iloc[:,:-1].to_numpy()) * n_docks  )\n",
    "    else:\n",
    "        y_pred = model.predict(data.iloc[:,:-1].to_numpy()) * n_docks\n",
    "    y_gold = data[\"bikes\"].to_numpy() * n_docks\n",
    "    \n",
    "    return mean_absolute_error(y_gold, y_pred)\n",
    "\n",
    "def reasonable_predictions(model, data):\n",
    "    y_pred = model.predict(data.to_numpy())\n",
    "    \n",
    "    y_pred = np.around(y_pred)\n",
    "    return y_pred\n",
    "\n",
    "def load_data():\n",
    "    df = []\n",
    "    \n",
    "    for path in Path('./Train/Train').rglob('*.csv'):\n",
    "        tmp = pd.read_csv(path)\n",
    "        df.append(tmp)\n",
    "    \n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    return clean_data(df)\n",
    "        \n",
    "def clean_data(df):\n",
    "    convert_weekdays(df)\n",
    "    # These three features do not change value throughout the entire training set\n",
    "    del df[\"month\"]\n",
    "    del df[\"year\"]\n",
    "    del df['precipitation.l.m2']\n",
    "    df = df.dropna(axis='rows')\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3c04e0",
   "metadata": {},
   "source": [
    "## New Features?\n",
    "Thought we could try making new features... people will use bikes more often when they aren't at work. Made a new feature that combines weekends and bank holidays? Might be more relevent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86fecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_hol_weekend(row):\n",
    "    if row['weekday'] == 6 or row['weekday'] == 7:\n",
    "        return 1\n",
    "    if row['isHoliday'] == 1:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "df['isOff'] = df.apply(is_hol_weekend,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e9ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"isOff\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ffa607",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"bikes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor(n_estimators=500, n_jobs=6)\n",
    "print(\"initialised\")\n",
    "forest.fit(df.iloc[:,:-1].to_numpy(), df[\"bikes\"].to_numpy())\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "imp_indixes = np.argsort(importances)[::-1]\n",
    "feature_order = df.columns[:-1][imp_indixes]\n",
    "importances = importances[imp_indixes]\n",
    "\n",
    "imp_df = pd.DataFrame(data = importances, index = feature_order, columns=[\"relative_importance\"])\n",
    "\n",
    "# print(score_abs_error(forest, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97097e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_ranked_10 = feature_order[-5:]\n",
    "for feature in lowest_ranked_10:\n",
    "    del df[feature]\n",
    "print(imp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c5bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eac795",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(df, test_size=0.2)\n",
    "\n",
    "\n",
    "columns = list(df.columns[-6:])\n",
    "print(columns)\n",
    "for c in columns:\n",
    "    train[c] = train[c].to_numpy() / train[\"numDocks\"].to_numpy()\n",
    "    val[c] = val[c].to_numpy() / val[\"numDocks\"].to_numpy()\n",
    "\n",
    "\n",
    "train_docks = train[\"numDocks\"]\n",
    "val_docks = val[\"numDocks\"]\n",
    "del train[\"numDocks\"]\n",
    "del val[\"numDocks\"]\n",
    "\n",
    "val\n",
    "\n",
    "# See all Rows/Cols\n",
    "\n",
    "#pd.set_option('display.max_rows', 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa579347",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75561d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a18a5553",
   "metadata": {},
   "source": [
    "# Random elimination parameter tuning\n",
    "## Random forest regressor\n",
    "\n",
    "This cell uses `HalvingRandomSearchCV` to find near-optimal parameters for a random forest regressor. It takes a while to run with these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8859a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import time\n",
    "# from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "# from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n",
    "# from scipy.stats import randint\n",
    "\n",
    "\n",
    "# # forest_boost = GradientBoostingRegressor(n_estimators=n_est, loss='squared_error', learning_rate=0.2, max_depth=2, verbose=1)\n",
    "# #  forest_boost = SVR()\n",
    "# searched_boost = RandomForestRegressor()\n",
    "# searched_boost = GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "\n",
    "# param_distributions = {\"max_depth\":  [2,3,4],   #, 5, 6, None],\n",
    "#                        \"min_samples_split\": np.around(np.linspace(2,20,10)).astype(np.int32),\n",
    "#                        \"learning_rate\": np.linspace(0.0001,1,10),\n",
    "#                        \"n_estimators\": np.linspace(5, 5000, 50).astype(np.int32)\n",
    "#                       }\n",
    "\n",
    "# search = HalvingRandomSearchCV(searched_boost, param_distributions,\n",
    "#                                resource='n_samples', aggressive_elimination=True, min_resources = 1000,\n",
    "#                                factor = 2 ,cv = 2,    # n_candidates =  25\n",
    "#                                random_state=0, verbose=1, n_jobs=6).fit(train.iloc[:,:-1].to_numpy(), train[\"bikes\"].to_numpy())\n",
    "\n",
    "# print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "print(\"initialised\")\n",
    "#forest_boost = GradientBoostingRegressor(**search.best_params_)\n",
    "forest_boost = GradientBoostingRegressor(n_estimators= 5000, min_samples_split= 10, max_depth= 3, learning_rate= 0.2, verbose = 1)\n",
    "forest_boost.fit(train.iloc[:,:-1].to_numpy(), train[\"bikes\"].to_numpy())\n",
    "print(f'fitted in {time() - start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ae98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(score_abs_error(forest_boost, val, val_docks))\n",
    "print(score_abs_error(forest_boost, val, val_docks, round_ = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# results = pd.DataFrame(search.cv_results_)\n",
    "# results[\"params_str\"] = results.params.apply(str)\n",
    "# params = search.param_distributions\n",
    "# # results.drop_duplicates(subset=(\"params_str\", \"iter\"), inplace=True)\n",
    "# learning_rates = params[\"learning_rate\"]\n",
    "# mean_scores = results.pivot(\n",
    "#     index=\"iter\", columns=\"params_str\", values=\"mean_test_score\"\n",
    "# )\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(16,12))\n",
    "# mean_scores.plot(legend=False, alpha=0.6, ax = ax, linewidth=8)\n",
    "\n",
    "# labels = [\n",
    "#     f\"iter={i}\\nn_samples={search.n_resources_[i]} \\nn_candidates={search.n_candidates_[i]}\"# \\nn_estimators={params[\"n_estimators\"][i]} \"\n",
    "#     for i in range(search.n_iterations_)\n",
    "# ]\n",
    "\n",
    "# ax.set_xticks(range(search.n_iterations_))\n",
    "# ax.set_xticklabels(labels, rotation=45, multialignment=\"left\")\n",
    "# ax.set_title(\"Scores of candidates over iterations\")\n",
    "# ax.set_ylabel(\"mean test score\", fontsize=15)\n",
    "# ax.set_xlabel(\"iterations\", fontsize=15)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956bd375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     n_est = search.best_params_[\"n_estimators\"]\n",
    "# except:\n",
    "n_est = 5000\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "test_score = np.zeros((n_est,), dtype=np.float64)\n",
    "\n",
    "y_test = val[\"bikes\"]\n",
    "#y_pred = reasonable_predictions(forest_boost, val.iloc[:, :-1])\n",
    "\n",
    "for i, y_pred in enumerate(forest_boost.staged_predict(val.iloc[:,:-1])):\n",
    "    test_score[i] = mean_absolute_error(y_test* val_docks, y_pred* val_docks)\n",
    "    #test_score[i] = forest_boost.loss_(y_test, np.around(y_pred))\n",
    "\n",
    "#test_score = test_score * val_docks\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title(\"Training and validation error\")\n",
    "plt.plot(\n",
    "    np.arange(n_est) + 1,\n",
    "    forest_boost.train_score_,\n",
    "    \"b-\",\n",
    "    label=\"Training Set\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(n_est) + 1, test_score, \"r-\", label=\"Validation Set\"\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting Iterations\")\n",
    "plt.ylabel(\"Squared error\")\n",
    "#plt.yscale('log')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(y_test - forest_boost.predict(val.iloc[:,:-1]), bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc564e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "ids = test[\"Id\"]\n",
    "\n",
    "del test[\"Id\"]\n",
    "del test[\"month\"]\n",
    "del test[\"year\"]\n",
    "\n",
    "test.dropna()\n",
    "\n",
    "convert_weekdays(test)\n",
    "# test[test.columns] = scaler.fit_transform(test[test.columns])\n",
    "\n",
    "# for feature in lowest_ranked_10:\n",
    "#     del test[feature]\n",
    "\n",
    "\n",
    "columns = list(test.columns[-5:])\n",
    "print(columns)\n",
    "for c in columns:\n",
    "    test[c] = test[c].to_numpy() / test[\"numDocks\"].to_numpy()\n",
    "\n",
    "print(test.head())\n",
    "\n",
    "plt.hist(test[\"bikes_3h_ago\"])\n",
    "plt.show()\n",
    "\n",
    "test_docks = test[\"numDocks\"].to_numpy()\n",
    "print(test_docks)\n",
    "del test[\"numDocks\"]\n",
    "\n",
    "\n",
    "#y_pred = forest_boost.predict(test)\n",
    "\n",
    "print(np.around(forest_boost.predict(test.to_numpy()) * test_docks))\n",
    "\n",
    "y_pred = np.around(forest_boost.predict(test.to_numpy()) * test_docks)\n",
    "print(repr(y_pred))\n",
    "sub_df = pd.DataFrame(data=y_pred, index = ids, columns = [\"bikes\"])\n",
    "\n",
    "sub_df.index.name = 'Id'\n",
    "\n",
    "print(sub_df.head())\n",
    "print(sub_df.shape)\n",
    "\n",
    "sub_df.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd78c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
